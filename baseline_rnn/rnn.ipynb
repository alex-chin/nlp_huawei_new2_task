{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "from torch.nn import Module, Embedding, LSTM, RNN, GRU, Linear, Sequential, Dropout\n",
    "from torch.nn.utils.rnn import PackedSequence, pack_sequence\n",
    "from torch.nn.functional import sigmoid, relu, elu, tanh\n",
    "from numpy import asarray\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word_pattern=\"[\\w']+\"):\n",
    "        \"\"\"\n",
    "        Simple tokenizer that splits the sentence by given regex pattern\n",
    "        :param word_pattern: pattern that determines word boundaries\n",
    "        \"\"\"\n",
    "        self.word_pattern = re.compile(word_pattern)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.word_pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokenized_texts: List[List[str]], max_vocab_size=None):\n",
    "        \"\"\"\n",
    "        Builds a vocabulary by concatenating all tokenized texts and counting words.\n",
    "        Most common words are placed in vocabulary, others are replaced with [UNK] token\n",
    "        :param tokenized_texts: texts to build a vocab\n",
    "        :param max_vocab_size: amount of words in vocabulary\n",
    "        \"\"\"\n",
    "        counts = Counter(chain(*tokenized_texts))\n",
    "        max_vocab_size = max_vocab_size or len(counts)\n",
    "        common_pairs = counts.most_common(max_vocab_size)\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.EOS_IDX = 2\n",
    "        self.itos = [\"<PAD>\", \"<UNK>\", \"<EOS>\"] + [pair[0] for pair in common_pairs]\n",
    "        self.stoi = {token: i for i, token in enumerate(self.itos)}\n",
    "\n",
    "    def vectorize(self, text: List[str]):\n",
    "        \"\"\"\n",
    "        Maps each token to it's index in the vocabulary\n",
    "        :param text: sequence of tokens\n",
    "        :return: vectorized sequence\n",
    "        \"\"\"\n",
    "        return [self.stoi.get(tok, self.UNK_IDX) for tok in text]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.itos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels, vocab: Vocab):\n",
    "        \"\"\"\n",
    "        A Dataset for the task\n",
    "        :param tokenized_texts: texts from a train/val/test split\n",
    "        :param labels: corresponding toxicity ratings\n",
    "        :param vocab: vocabulary with indexed tokens\n",
    "        \"\"\"\n",
    "        self.texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.vocab.vectorize(self.texts[item]) + [self.vocab.EOS_IDX], self.labels[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Technical method to form a batch to feed into recurrent network\n",
    "        \"\"\"\n",
    "        tmp = pack_sequence([torch.tensor(pair[0]) for pair in batch], enforce_sorted=False), torch.tensor(\n",
    "            [pair[1] for pair in batch])\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(Module):\n",
    "    def __init__(self, config: Dict, vocab: Vocab, emb_matrix):\n",
    "        \"\"\"\n",
    "        Baseline classifier, hyperparameters are passed in `config`.\n",
    "        Consists of recurrent part and a classifier (Multilayer Perceptron) part\n",
    "        Keys are:\n",
    "            - freeze: whether word embeddings should be frozen\n",
    "            - cell_type: one of: RNN, GRU, LSTM, which recurrent cell model should use\n",
    "            - hidden_size: size of hidden state for recurrent cell\n",
    "            - num_layers: amount of recurrent cells in the model\n",
    "            - cell_dropout: dropout rate between recurrent cells (not applied if model has only one cell!)\n",
    "            - bidirectional: boolean, whether to use unidirectional of bidirectional model\n",
    "            - out_activation: one of: \"sigmoid\", \"tanh\", \"relu\", \"elu\". Activation in classifier part\n",
    "            - out_dropout: dropout rate in classifier part\n",
    "            - out_sizes: List[int], hidden size of each layer in classifier part. Empty list means that final\n",
    "                layer is attached directly to recurrent part output\n",
    "        :param config: configuration of model\n",
    "        :param vocab: vocabulary\n",
    "        :param emb_matrix: embeddings matrix from `prepare_emb_matrix`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.embeddings = Embedding.from_pretrained(emb_matrix, freeze=config[\"freeze\"],\n",
    "                                                    padding_idx=vocab.PAD_IDX)\n",
    "        cell_types = {\n",
    "            \"RNN\": RNN,\n",
    "            \"GRU\": GRU,\n",
    "            \"LSTM\": LSTM}\n",
    "        cell_class = cell_types[config[\"cell_type\"]]\n",
    "        self.cell = cell_class(input_size=emb_matrix.size(1),\n",
    "                               batch_first=True,\n",
    "                               hidden_size=config[\"hidden_size\"],\n",
    "                               num_layers=config[\"num_layers\"],\n",
    "                               dropout=config[\"cell_dropout\"],\n",
    "                               bidirectional=config[\"bidirectional\"],\n",
    "                               )\n",
    "        activation_types = {\n",
    "            \"sigmoid\": sigmoid,\n",
    "            \"tanh\": tanh,\n",
    "            \"relu\": relu,\n",
    "            \"elu\": elu,\n",
    "        }\n",
    "        self.out_activation = activation_types[config[\"out_activation\"]]\n",
    "        self.out_dropout = Dropout(config[\"out_dropout\"])\n",
    "        cur_out_size = config[\"hidden_size\"] * config[\"num_layers\"]\n",
    "        if config[\"bidirectional\"]:\n",
    "            cur_out_size *= 2\n",
    "        out_layers = []\n",
    "        for cur_hidden_size in config[\"out_sizes\"]:\n",
    "            out_layers.append(Linear(cur_out_size, cur_hidden_size))\n",
    "            cur_out_size = cur_hidden_size\n",
    "        out_layers.append(Linear(cur_out_size, 6))\n",
    "        self.out_proj = Sequential(*out_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embeddings(input.data)\n",
    "        _, last_state = self.cell(PackedSequence(embedded,\n",
    "                                                 input.batch_sizes,\n",
    "                                                 sorted_indices=input.sorted_indices,\n",
    "                                                 unsorted_indices=input.unsorted_indices))\n",
    "        if isinstance(last_state, tuple):\n",
    "            last_state = last_state[0]\n",
    "        last_state = last_state.transpose(0, 1)\n",
    "        last_state = last_state.reshape(last_state.size(0), -1)\n",
    "        return self.out_proj(last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Fits end evaluates given model with Adam optimizer.\n",
    "         Hyperparameters are specified in `config`\n",
    "        Possible keys are:\n",
    "            - n_epochs: number of epochs to train\n",
    "            - lr: optimizer learning rate\n",
    "            - weight_decay: l2 regularization weight\n",
    "            - device: on which device to perform training (\"cpu\" or \"cuda\")\n",
    "            - verbose: whether to print anything during training\n",
    "        :param config: configuration for `Trainer`\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.n_epochs = config[\"n_epochs\"]\n",
    "        self.setup_opt_fn = lambda model: Adam(model.parameters(),\n",
    "                                               config[\"lr\"],\n",
    "                                               weight_decay=config[\"weight_decay\"])\n",
    "        self.model = None\n",
    "        self.opt = None\n",
    "        self.history = None\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.device = config[\"device\"]\n",
    "        self.verbose = config.get(\"verbose\", True)\n",
    "\n",
    "    def fit(self, model, train_loader, val_loader):\n",
    "        \"\"\"\n",
    "        Fits model on training data, each epoch evaluates on validation data\n",
    "        :param model: PyTorch model for toxic comments classification (for example, `RecurrentClassifier`)\n",
    "        :param train_loader: DataLoader for training data\n",
    "        :param val_loader: DataLoader for validation data\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model = model.to(self.device)\n",
    "        self.opt = self.setup_opt_fn(self.model)\n",
    "        self.history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print(f\"Epoch {epoch}/{self.n_epochs}\")\n",
    "            train_info = self._train_epoch(train_loader)\n",
    "            val_info = self._val_epoch(val_loader)\n",
    "            self.history[\"train_loss\"].extend(train_info[\"train_loss\"])\n",
    "            self.history[\"val_loss\"].append(val_info[\"loss\"])\n",
    "            self.history[\"val_acc\"].append(val_info[\"acc\"])\n",
    "        return self.model.eval()\n",
    "\n",
    "    def _train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        if self.verbose:\n",
    "            train_loader = tqdm(train_loader)\n",
    "        for batch in train_loader:\n",
    "            self.model.zero_grad()\n",
    "            texts, labels = batch\n",
    "            logits = self.model.forward(texts.to(self.device))\n",
    "            loss = self.loss_fn(logits, labels.to(self.device))\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            loss_val = loss.item()\n",
    "            if self.verbose:\n",
    "                train_loader.set_description(f\"Loss={loss_val:.3}\")\n",
    "            losses.append(loss_val)\n",
    "        return {\"train_loss\": losses}\n",
    "\n",
    "    def _val_epoch(self, val_loader):\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        if self.verbose:\n",
    "            val_loader = tqdm(val_loader)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts, labels = batch\n",
    "                logits = self.model.forward(texts.to(self.device))\n",
    "                all_logits.append(logits)\n",
    "                all_labels.append(labels)\n",
    "        all_labels = torch.cat(all_labels).to(self.device)\n",
    "        all_logits = torch.cat(all_logits)\n",
    "        loss = CrossEntropyLoss()(all_logits, all_labels).item()\n",
    "        acc = (all_logits.argmax(1) == all_labels).float().mean().item()\n",
    "        if self.verbose:\n",
    "            val_loader.set_description(f\"Loss={loss:.3}; Acc:{acc:.3}\")\n",
    "        return {\"acc\": acc, \"loss\": loss}\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"You should train the model first\")\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                texts, labels = batch\n",
    "                logits = self.model.forward(texts.to(self.device))\n",
    "                predictions.extend(logits.argmax(1).tolist())\n",
    "        return asarray(predictions)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"You should train the model first\")\n",
    "        checkpoint = {\"config\": self.model.config,\n",
    "                      \"trainer_config\": self.config,\n",
    "                      \"vocab\": self.model.vocab,\n",
    "                      \"emb_matrix\": self.model.emb_matrix,\n",
    "                      \"state_dict\": self.model.state_dict()}\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        ckpt = torch.load(path)\n",
    "        keys = [\"config\", \"trainer_config\", \"vocab\", \"emb_matrix\", \"state_dict\"]\n",
    "        for key in keys:\n",
    "            if key not in ckpt:\n",
    "                raise RuntimeError(f\"Missing key {key} in checkpoint\")\n",
    "        new_model = RecurrentClassifier(ckpt[\"config\"], ckpt[\"vocab\"], ckpt[\"emb_matrix\"])\n",
    "        new_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "        new_trainer = cls(ckpt[\"trainer_config\"])\n",
    "        new_trainer.model = new_model\n",
    "        new_trainer.model.to(new_trainer.device)\n",
    "        return new_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_frac=0.85):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test parts, stratifying by labels.\n",
    "    Should it shuffle the data before split?\n",
    "    :param data: dataset to split\n",
    "    :param train_frac: proportion of train examples\n",
    "    :return: texts and labels for each split\n",
    "    \"\"\"\n",
    "    n_toxicity_ratings = 6\n",
    "    train_labels = []\n",
    "    val_labels = []\n",
    "    train_texts = []\n",
    "    val_texts = []\n",
    "    for label in range(n_toxicity_ratings):\n",
    "        texts = data[data.target == label].movie_description.values\n",
    "        n_train = int(len(texts) * train_frac)\n",
    "        n_val = len(texts) - n_train\n",
    "        train_texts.extend(texts[:n_train])\n",
    "        val_texts.extend(texts[n_train:])\n",
    "        train_labels += [label] * n_train\n",
    "        val_labels += [label] * n_val\n",
    "    return train_texts, train_labels, val_texts, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_emb_matrix(gensim_model, vocab: Vocab):\n",
    "    \"\"\"\n",
    "    Extract embedding matrix from Gensim model for words in Vocab.\n",
    "    Initialize embeddings not presented in `gensim_model` randomly\n",
    "    :param gensim_model: W2V Gensim model\n",
    "    :param vocab: vocabulary\n",
    "    :return: embedding matrix\n",
    "    \"\"\"\n",
    "    mean = gensim_model.vectors.mean(1).mean()\n",
    "    std = gensim_model.vectors.std(1).mean()\n",
    "    vec_size = gensim_model.vector_size\n",
    "    emb_matrix = torch.zeros((len(vocab), vec_size))\n",
    "    for i, word in enumerate(vocab.itos[1:], 1):\n",
    "        try:\n",
    "            emb_matrix[i] = torch.tensor(gensim_model.get_vector(word))\n",
    "        except KeyError:\n",
    "            emb_matrix[i] = torch.randn(vec_size) * std + mean\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(model, comment):\n",
    "    tok_text = tok.tokenize(comment)\n",
    "    indexed_text = torch.tensor(vocab.vectorize(tok_text)).to(t.device)\n",
    "    rating = model(pack_sequence([indexed_text])).argmax().item()\n",
    "    print(f\"Toxicity rating for \\\"{comment}\\\" is: {rating}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_description</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hellraiser</td>\n",
       "      <td>A new take on Clive Barker's 1987 horror class...</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hocus Pocus 2</td>\n",
       "      <td>It's been 29 years since someone lit the Black...</td>\n",
       "      <td>Kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X</td>\n",
       "      <td>In 1979, a group of young filmmakers set out t...</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Piggy</td>\n",
       "      <td>With the summer sun beating down on her rural ...</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deadstream</td>\n",
       "      <td>After a public controversy left him disgraced ...</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie_name                                  movie_description  target\n",
       "0     Hellraiser  A new take on Clive Barker's 1987 horror class...  Horror\n",
       "1  Hocus Pocus 2  It's been 29 years since someone lit the Black...    Kids\n",
       "2              X  In 1979, a group of young filmmakers set out t...  Horror\n",
       "3          Piggy  With the summer sun beating down on her rural ...  Horror\n",
       "4     Deadstream  After a public controversy left him disgraced ...  Horror"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../train.csv\")\n",
    "train = train[train[\"movie_description\"].notna()]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_description</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hellraiser</td>\n",
       "      <td>A new take on Clive Barker's 1987 horror class...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hocus Pocus 2</td>\n",
       "      <td>It's been 29 years since someone lit the Black...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X</td>\n",
       "      <td>In 1979, a group of young filmmakers set out t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Piggy</td>\n",
       "      <td>With the summer sun beating down on her rural ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deadstream</td>\n",
       "      <td>After a public controversy left him disgraced ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie_name                                  movie_description  target\n",
       "0     Hellraiser  A new take on Clive Barker's 1987 horror class...       3\n",
       "1  Hocus Pocus 2  It's been 29 years since someone lit the Black...       4\n",
       "2              X  In 1979, a group of young filmmakers set out t...       3\n",
       "3          Piggy  With the summer sun beating down on her rural ...       3\n",
       "4     Deadstream  After a public controversy left him disgraced ...       3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train[\"target\"])\n",
    "train[\"target\"] = le.transform(train[\"target\"])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok_texts = [tok.tokenize(t) for t in train.movie_description.values]\n",
    "vocab = Vocab(tok_texts, max_vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, val_texts, val_labels = train_test_split(train)\n",
    "train_dataset = TextDataset([tok.tokenize(t) for t in train_texts], train_labels, vocab)\n",
    "val_dataset = TextDataset([tok.tokenize(t) for t in val_texts], val_labels, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GENSIM_DATA_DIR\"] = str(Path.cwd())\n",
    "gensim_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "emb_matrix = prepare_emb_matrix(gensim_model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"freeze\": True,\n",
    "    \"cell_type\": \"LSTM\",\n",
    "    \"cell_dropout\": 0.3,\n",
    "    \"num_layers\": 2,\n",
    "    \"hidden_size\": 128,\n",
    "    \"out_activation\": \"relu\",\n",
    "    \"bidirectional\": False,\n",
    "    \"out_dropout\": 0.2,\n",
    "    \"out_sizes\": [200],\n",
    "    # \"verbose\": False,\n",
    "}\n",
    "\n",
    "trainer_config = {\n",
    "    \"lr\": 3e-4,\n",
    "    \"n_epochs\": 10,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"batch_size\": 128,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "clf_model = RecurrentClassifier(config, vocab, emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a3dd2f066c468ea6271342ab33374a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98bd6400e2647418b8dad4619262ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c81c1fdff34064b22b1cd97f465b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a962a53f2045447faae4be10f8079d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113b338e2c6b4a028917568441748161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b8b92d912c423caea58b2993670bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e67938a5ca45b5b3da25a46d76ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c801e7030bd40f4a0d562ccdfb0529d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b450a4dc69ba488c9468174b0d050da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222717d340e349ab88dbd52b4633ca9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8e7fed3c6b48b2a98e844bf3f5ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314706d2127d489e83d19efd6c2baf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6aa5c845fdc40cbb529c8e0e77b0abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b018214817da45f185c919e793e2598a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e841691bfc214631aba81bfaafefa25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bb10d421e54315952db846802e1d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f673fae1b9bd4e5eb91c7a9330f2255b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa8659dbae24dc2a8c05a9e0ef5962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a5b1a39991417faac71c49715f0f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d2f37ea61043cb939e9d77463b61a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RecurrentClassifier(\n",
       "  (embeddings): Embedding(30003, 100, padding_idx=0)\n",
       "  (cell): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (out_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (out_proj): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=trainer_config[\"batch_size\"],\n",
    "                              shuffle=True,\n",
    "                              num_workers=0,\n",
    "                              collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=trainer_config[\"batch_size\"],\n",
    "                            shuffle=False,\n",
    "                            num_workers=0,\n",
    "                            collate_fn=val_dataset.collate_fn)\n",
    "t = Trainer(trainer_config)\n",
    "t.fit(clf_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(\"baseline_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trainer.load(\"baseline_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity rating for \"Please sir do not delete my edits\" is: 4\n",
      "Toxicity rating for \"They are nazi pal, forget it\" is: 1\n",
      "Toxicity rating for \"You suck\" is: 4\n"
     ]
    }
   ],
   "source": [
    "predict_toxicity(t.model, \"Please sir do not delete my edits\")\n",
    "predict_toxicity(t.model, \"They are nazi pal, forget it\")\n",
    "predict_toxicity(t.model, \"You suck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Harrigan's Phone</td>\n",
       "      <td>When Craig, a young boy living in a small town...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To Leslie</td>\n",
       "      <td>Leslie (Andrea Riseborough) is a West Texas si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hellraiser</td>\n",
       "      <td>Sexual deviant Frank (Sean Chapman) inadverten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You Won't Be Alone</td>\n",
       "      <td>Set in an isolated mountain village in 19th ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Confess, Fletch</td>\n",
       "      <td>In this delightful comedy romp, Jon Hamm stars...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             movie_name                                  movie_description\n",
       "0  Mr. Harrigan's Phone  When Craig, a young boy living in a small town...\n",
       "1             To Leslie  Leslie (Andrea Riseborough) is a West Texas si...\n",
       "2            Hellraiser  Sexual deviant Frank (Sean Chapman) inadverten...\n",
       "3    You Won't Be Alone  Set in an isolated mountain village in 19th ce...\n",
       "4       Confess, Fletch  In this delightful comedy romp, Jon Hamm stars..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader( TextDataset([tok.tokenize(t) for t in test.movie_description.values], [-1] * test.shape[0], vocab), \n",
    "                            batch_size=trainer_config[\"batch_size\"],\n",
    "                            shuffle=False,\n",
    "                            num_workers=0,\n",
    "                            collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "predictions = t.predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mystery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target\n",
       "0   Horror\n",
       "1    Drama\n",
       "2     Kids\n",
       "3   Horror\n",
       "4  Mystery"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"../sample_submission.csv\")\n",
    "sample_submission[\"target\"] = le.inverse_transform(predictions)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ce47e61e2a197e18f6178d2b52c72741babf3e522a6ab2c8e52c7be76b98a41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
